{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Current","text":"<p>TODO:</p>"},{"location":"#general","title":"General","text":"<ul> <li> <p> Discuss the idea of changing the parsing scripts to the new method</p> </li> <li> <p> Go over SLAC EVNT Log File with Mike</p> </li> <li> Think about the structure of the Coffea parsing script (the merge request will help when parsing; will just need to find that specific block of text and parse; otherwise I would just be missing the block of text all the time):</li> <li> Giordon will update the script via a merge request, see example below    <pre><code>{'bytesread': 6392230, 'columns': ['ph_select_tightID_NOSYS', 'ph_pt_NOSYS', 'ph_isEM_NOSYS'], 'entries': 191297, 'processtime': 1.8725175857543945, 'chunks': 2}\n</code></pre></li> <li> Learn how to use PDB</li> <li> Learn how to use Pyinstrument</li> <li> Look into version control</li> <li> Update the FF versions used at all the sites; have their versions match</li> </ul>"},{"location":"#bnl","title":"BNL","text":"<ul> <li> Update the FF code so that it matches what is being used at SLAC</li> </ul>"},{"location":"#slac","title":"SLAC","text":""},{"location":"#uc","title":"UC","text":"<ul> <li> Parse Ntuple -&gt; Hist log files and compare both FF and Coffea</li> <li> Update the FF code so that it matches what is being used at SLAC</li> </ul>"},{"location":"#nersc","title":"NERSC","text":"<ul> <li> Update the FF code so that it matches what is being used at SLAC</li> </ul>"},{"location":"#site","title":"Site","text":"<ul> <li> Document the structure of the benchmarking job; do so for all sites; batch and interactive container jobs; documenting the parsing script as well; it should help people navigate the GitHub page; explain where the cron files are deployed at the various sites; basically explain everything I've done over the past year</li> <li> Call out where the user would go for an interactive set up for the environment and whatnot</li> </ul>"},{"location":"#dumps","title":"Dumps","text":"<p>Dump - PYAR</p> <p>Dump - mkdocs</p> <p>Dump - Meetings</p> <p>Dump - Updates on Kibana</p>"},{"location":"#projects","title":"Projects","text":"<p>Project - Coffea Framework Ntuple to Hist</p> <p>Project - FF Framework Ntuple to Hist</p> <p>Project - Event Loop Ntuple to Hist</p> <p>Project - Parsing Script</p>"},{"location":"#ideas","title":"Ideas","text":"<ul> <li>Email grad advisors and figure out if there will be openings for different programs and their application cycles TOP ask about TA</li> </ul> <p>DONE:</p> <p>Completed Tasks</p>"},{"location":"BNL%20-%20EVNT%20Dumps/","title":"BNL - EVNT Dumps","text":""},{"location":"BNL%20-%20EVNT%20Dumps/#el9","title":"EL9","text":"<ul> <li> <p>This job needs to have another function that opens the split.log file and extracts the host name and payload size -&gt; Didn't make a new function just ran everything under the same function</p> </li> <li> <p>Currently deciding how to split the instances that have been sent, the whole set of entries since the beginning, and the ones that were sent ~6 hours ago</p> </li> </ul>"},{"location":"BNL%20-%20Rucio%20Dumps/","title":"BNL - Rucio Dumps","text":"<ul> <li>When running this I discovered an issue; if the job fails and can't append to the file \"new_current_list.txt\" there won't be any entry to look at the next time it runs so it wont' be able to send information. Found a quick fit but will have to tackle this problem later</li> <li>This job is running and hasn't had an issue since this morning</li> </ul>"},{"location":"Completed%20Tasks/","title":"2025","text":"<ul> <li> Store copies of the crontab/scrontab files used at the AFs in GitHub</li> <li> Need to check the two coffea.root files found in my home directory at UC-AF:</li> <li> Make notes or reference for the root code that make the histogram</li> <li> Need to check on the script that sends information to the dashboard at all sites and continue to send information from BNL</li> <li> Make slides with the diagram found in my notes:</li> <li> Learn how to use mkdocs</li> <li> Use mkdocs mermaid:<ul> <li> Should be able to use markdown syntax as shown here</li> </ul> </li> <li> Fix the Rucio download error; Agree to the updated the AUP</li> <li> Continue to work on the BNL Parsing Scripts:</li> <li> Check the error caused by the Rucio Job -&gt; Seems to be an error with the job script itself -&gt; Error was with the directory containing previously contained files was missing so \"rm -r dir/\" resulted in an error</li> <li> Log into SLAC after maintenance and work on parsing scripts:</li> <li> Check again later today; checked this morning at 9AM and connection was closing</li> <li> Check the crontab file at SLAC</li> <li> Fix the coffea job</li> <li> Update and schedule the FF job</li> <li> Make bubble going from ntuple-&gt;Coffea-&gt;Hist NOT physlite-&gt;coffea-&gt;hist</li> <li> SLHA -&gt; GenTF -&gt; EVNT</li> <li> EVNT -&gt; RecoTF -&gt; DAOD</li> <li> PHYSLITE -&gt; Rucio Download</li> <li> RHS-Top three can be collapsed into one</li> <li> Remove line from DAOD_TRUTH to event loop (no job running on DAOD_TRUTH)</li> <li> Update and link the GenTF and Reco_TF/Derivation_TF nodes</li> <li> Fix the parsing scripts</li> <li> Figure out what is causing this error: <pre><code>OSError: [Errno 121] Remote I/O error\n</code></pre> ChatGPT is suggesting this is a hardware or system-level issue and not a bug in the code. I figured it out and it was really the Rucio parsing script that was holding all other jobs behind because of how I have the parsing scripts scheduled.</li> <li> Set up the SLAC FF -&gt; I need to figure out why I'm getting a type error when inputting the config file at SLAC</li> <li> Get to the bottom of this on PDB, insert break points and see why the error is being thrown</li> <li> Check the paths and make sure it is still available</li> <li> If there is a file that is causing the issue, wrap it using a try-except and have it give an error message and give me the name of the file that is missing/faulty</li> <li> Keep the dashboard populated and don't move until it is guaranteed that there won't be issue</li> <li> Check on the jobs; they aren't posting to Kibana -&gt; Not sure why they aren't posting, the script works when I run it without using crontab</li> <li> Compare Coffea, FF, and EventLoop histogram</li> <li> Schedule EVNT Parsing scripts</li> <li> Schedule TRUTH3 Parsing scripts:</li> <li> Copy the usual files to the respective directories:<ul> <li> Native</li> <li> EL9</li> <li> CentOS7</li> <li> Native int</li> <li> EL9 int </li> <li> CentOS7 int</li> </ul> </li> <li> Write the main script for the respective jobs:<ul> <li> Native</li> <li> EL9</li> <li> CentOS7</li> <li> Native int</li> <li> EL9 int </li> <li> CentOS7 int</li> </ul> </li> <li> Create the \"current_list.txt\" file for the respective jobs:<ul> <li> Native</li> <li> EL9</li> <li> CentOS7</li> <li> Native int</li> <li> EL9 int </li> <li> CentOS7 int</li> </ul> </li> <li> Add a legend to the flowchart -- red is scripts and blue is data formats (stored on disk, root file or pdf)</li> </ul>"},{"location":"Dump%20-%20Meetings/","title":"Meeting with Mike","text":"<ul> <li>Benchmarking Event Loop</li> </ul>"},{"location":"Dump%20-%20Meetings/#gpu-benchmarking-job","title":"GPU Benchmarking Job","text":"<ul> <li>Check if there's a standard GPU benchmark; use Ntuple</li> <li>Use one of the Physics 152 NB that is a CNN or GAN and see if I can run at UC and BNL (batch)</li> <li> <p>Make it a standalone flowchart; consider incorporating into the rest of the flowchart</p> </li> <li> <p>Ask grad school advisor about openings in their institutions</p> </li> </ul>"},{"location":"Dump%20-%20Meetings/#python-file-for-event-loop","title":"Python File for EVENT LOOP","text":"<ul> <li> Run the python EVent loop produce a histogram in a root file and then compare with the other jobs FF and Coffea:</li> <li> Implement into a job and run in the batch; compare the run time coffea and FF</li> <li> Have the average over x amount of days comparing the run time</li> <li> Report the code once it is done so that it can be faster; accessing arrays Pre-ID branches</li> <li> <p>TOP PRIORITY</p> </li> <li> <p>Separate github for the \"Provide an updated quickstart guide for UC-AF, following SW carpentries tutorial format\" milestone; follow the same format that IRIS-HEP has</p> </li> <li>It can be personal and show the carpentry material</li> <li>Update the carpentry</li> </ul>"},{"location":"Dump%20-%20PYAR/","title":"Dump   PYAR","text":"<p>= Improvements =  - Need to relate the coordinate systems and their importance to Particle Physics at the end of Notebook 3</p> <ul> <li>Don't remove anything from the notebooks she will include requirements when sending the fliers</li> </ul>"},{"location":"Dump%20-%20mkdocs/","title":"Dump   mkdocs","text":"<p>Notes obtained from: Materials for MkDocs</p>"},{"location":"Dump%20-%20mkdocs/#installation","title":"Installation","text":"<ul> <li>Can be installed using pip:   <pre><code>pip install mkdocs-material\n</code></pre></li> <li>Automatically installs compatible versions of all dependencies</li> <li>Can be install using git:</li> <li>cloning the repo:   <pre><code>git clone https://github.com/squidfunk/mkdocs-material.git\n</code></pre></li> <li>Install the themes and its dependencies:   <pre><code>pip install -e mkdocs-material\n</code></pre></li> </ul>"},{"location":"Dump%20-%20mkdocs/#creating-site","title":"Creating site","text":"<ul> <li>Go to the directory where you want your project to be located and enter: <pre><code>  mkdocs new .\n</code></pre></li> </ul>"},{"location":"Dump%20-%20mkdocs/#configuration","title":"Configuration","text":"<ul> <li>Minimal Configuration</li> <li>Advances Configuration</li> <li>They also provide templates found here</li> </ul>"},{"location":"Dump%20-%20mkdocs/#previewing","title":"Previewing","text":"<ul> <li>Run:</li> <li>mkdocs serve to run the server after saving</li> <li>Run:</li> <li>mkdocs serve --dirtyreload to see only the current page (assuming you have a lot of files that need to be rebuilt)</li> </ul>"},{"location":"Dump%20-%20mkdocs/#building-site","title":"Building Site","text":"<ul> <li>After editing you can build a static version of your site from your markdown files:</li> <li>mkdocs build</li> </ul>"},{"location":"Dump%20-%20mkdocs/#publishing","title":"Publishing","text":"<ul> <li>Suggests hosting code on GitHub</li> <li>Suggests using GitHub Actions to automate the deployment of the project documentation This is what I need to learn now</li> <li>Create a new GitHub Actions and copy this:</li> <li>when a new commit is pushed to either master or main, the site is built and deployed</li> <li>Using MkDocs:</li> <li>From within the directory containing mkdocs.yml run:   <pre><code>mkdocs gh-deploy --force\n</code></pre></li> </ul>"},{"location":"Parsing%20Script%20-%20BNL/","title":"Parsing Script - BNL","text":"<ul> <li>Focusing on this site</li> <li>Crontab file is in attsub02</li> </ul>"},{"location":"Parsing%20Script%20-%20BNL/#example-structure","title":"Example Structure","text":"<p>The example structure is from the 'Rucio Download' job, all other scripts will follow a similar sequence of steps.</p> <ul> <li>Path to Log Files: \"/atlasgpfs01/usatlas/data/jroblesgo/benchmarks\"</li> <li>Job Name: \"Rucio\"</li> <li>Name of Log File: \"rucio.log\"</li> <li>AF Site: \"bnl\"</li> <li>Script's Directory: \"/atlasgpfs01/usatlas/data/jroblesgo/parsing_jobs/rucio\"</li> <li>Old Entries Sent: \"rucio_sent.txt\"</li> <li> <p>File Containing Errors: \"rucio_error.txt\"</p> </li> <li> <p>Line 8-27 initialize the input variables: job name, file names, script path, and AF site</p> </li> <li>Line 30 initializes the data handling class using the previously defined variables</li> <li>Line 33 obtains a list of the paths, example: '/atlasgpfs01/usatlas/data/jroblesgo/benchmarks/2025.07.07T19'</li> <li>Line 36 obtains a list of paths to the job files, example: '/atlasgpfs01/usatlas/data/jroblesgo/benchmarks/2025.07.01T18/Rucio/rucio.log'</li> <li>Line 38-49 is a for loop that will populate a list with dictionaries containing the parsed data</li> <li>Line 53-57 obtains the submit time, in epochs, of the last entry sent to the dashboard</li> <li>Line 62-64 runs a for-loop over the list of dictionaries looking for the index of the dictionary that contains the previously acquired submit time</li> <li>Line 68-71 run a for-loop and creates a new list containing dictionaries starting from the i+1 index obtained in the previous step</li> <li>Line 75 sends the information to the dashboard using the previously obtained list</li> <li>Line 77-83 appends the dictionaries that were just sent to a file</li> </ul>"},{"location":"Parsing%20Script%20-%20BNL/#rucio-download","title":"Rucio Download","text":"<p>BNL - Rucio Dumps</p>"},{"location":"Parsing%20Script%20-%20BNL/#evnt","title":"EVNT","text":"<p>BNL - EVNT Dumps</p>"},{"location":"Parsing%20Script%20-%20BNL/#already-being-parsed","title":"Already Being Parsed","text":"<ul> <li> Rucio Job</li> <li> EVNT:</li> <li> EL9:</li> <li> CentOS7</li> <li> TRUTH3:</li> <li> EL9</li> <li> CentOS7</li> <li> Ntuple-&gt;Hist:</li> <li> FF</li> <li> Coffea</li> </ul>"},{"location":"Parsing%20Script%20-%20SLAC/","title":"Parsing Script - SLAC","text":"<ul> <li>Site was down for maintenance will log back in on 7/8/2025</li> </ul>"},{"location":"Project%20-%20Coffea%20Framework%20Ntuple%20to%20Hist/","title":"Coffea Framework Ntuple to Hist","text":"<ul> <li>Based on [[https://docs.google.com/document/d/1pg_Ws8pkJAnU29qfHEiqUENpD7sTrM2y0UX5ISvCjyI/edit?tab=t.80l00hjza16|this]], the only site that need to have their iteration of this job updated is NERSC </li> </ul>"},{"location":"Project%20-%20Coffea%20Framework%20Ntuple%20to%20Hist/#dump","title":"Dump","text":"<ul> <li>Working on getting the input files downloaded at NERSC:</li> <li>Had to fix some merge conflicts between what was in my desktop and my laptop</li> <li>I'm running into the same error as the other sites; release versions don't match</li> <li>Will ask Giordon about it during our meeting later</li> <li>Working on updating all of the errors I have encountered at the sites</li> <li>Will just have to wait for my meeting with Giordon</li> </ul> <p>TODO:</p> <ul> <li> I need to reinstall the ff packages for sites the following sites(I will simply use the code that I have for NERSC since that one works already):</li> <li> NERSC</li> <li> BNL</li> <li> SLAC</li> </ul>"},{"location":"Project%20-%20Event%20Loop%20Ntuple%20to%20Hist/","title":"Event Loop Ntuple to Hist","text":"<ul> <li> Need to set this job up in a container and submitted to the batch at all sites. (DOUBLE CHECK THAT IT IS RUNNING AT ALL SITES)</li> </ul>"},{"location":"Project%20-%20FF%20Framework%20Ntuple%20to%20Hist/","title":"FF Framework Ntuple to Hist","text":"<ul> <li>Need to set this job at all sites:</li> <li> <p> UC:</p> <ul> <li> Schedule the job</li> </ul> </li> <li> <p> BNL:</p> <ul> <li> Update the analysis file; should include the ID file, campaign and path to the input files:</li> <li> These files need to only have the mc20e campaign</li> <li> Update the path to the files</li> <li> Make sure the files have a \u201c.hist-output.root\u201d extension when running the script that will generate the weights</li> <li> Make sure the ANALYSIS.root files are listed in the file list .txt file</li> <li> Run the FF helping scripts that will produce the sum of weights from the analysis file list</li> <li> Update the config file:</li> <li> Update paths:<ul> <li> input files</li> <li> input file list</li> <li> output</li> </ul> </li> <li> Copy the input files to the appropriate directory</li> <li> Schedule the job</li> </ul> </li> <li> <p> SLAC:</p> <ul> <li> Update the analysis file; should include the ID file, campaign and path to the input files</li> <li> These files need to only have the mc20e campaign</li> <li> Update the path to the files</li> <li> Make sure the files have a \u201c.hist-output.root\u201d extension when running the script that will generate the weights</li> <li> Make sure the ANALYSIS.root files are listed in the file list .txt file</li> <li> Update the config file:</li> <li> Update paths:<ul> <li> input files</li> <li> input file list</li> <li> output</li> </ul> </li> <li> Run the FF helping scripts that will produce the sum of weights from the analysis file list</li> <li> Copy the input files to the appropriate directory</li> <li> Schedule the job</li> </ul> </li> <li> <p> NERSC:</p> <ul> <li> Update the analysis file; should include the ID file, campaign and path to the input files</li> <li> Make sure the files have a \u201c.hist-output.root\u201d extension when running the script that will generate the weights</li> <li> Make sure the ANALYSIS.root files are listed in the file list .txt file</li> <li> Run the FF helping scripts that will produce the sum of weights from the analysis file list</li> <li> Schedule the job</li> </ul> </li> </ul>"},{"location":"Project%20-%20Parsing%20Script/","title":"Parsing Script","text":"<p>Parsing Script - BNL</p> <p>Parsing Script - SLAC</p> <p>Parsing Script - UC</p> <p>Parsing Script - NERSC</p> <p>TODO:</p> <ul> <li> Email Ilija and arrange a meeting with him ~ 15 minutes or so to discuss and set up what I want to do with the data sent from the AFs (display them on the dashboard)</li> </ul> <p>TODO LATER:</p> <ul> <li> Send information using the authentication token:</li> <li> Update the parsing scripts so they utilize this method</li> <li> Test the optimization by checking its run-time</li> </ul>"}]}